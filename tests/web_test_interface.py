"""tests/web_test_interface.py

Gradio-based web interface for testing brAIniac components in the browser.
Provides interactive testing of chat engine, memory management, and tools.
"""

from __future__ import annotations

# Standard Library
import sys
import json
from unittest.mock import Mock, patch

# Third-Party Libraries
import gradio as gr

# Local Modules
from core.chat import ChatEngine
from core.memory import RollingMemory


def create_mock_engine() -> ChatEngine:
    """Create a ChatEngine with mocked Ollama client for testing.

    Returns:
        ChatEngine instance with mock Ollama client.
    """
    with patch("core.chat.Client") as mock_client_class:
        mock_client = Mock()
        mock_client.chat.return_value = {
            "message": {
                "role": "assistant",
                "content": (
                    "This is a **test response** from the mock LLM. "
                    "In production, this would be generated by Ollama."
                ),
            },
            "model": "llama3.1:8b-instruct-q4_K_M",
            "done": True,
        }
        mock_client_class.return_value = mock_client

        engine = ChatEngine()
        # Replace the client with our mock
        engine.client = mock_client
        return engine


# Global engine instance for the web interface
test_engine: ChatEngine = create_mock_engine()


def test_chat(message: str, history: list[list[str]]) -> tuple[str, list[list[str]]]:
    """Handle chat interaction for Gradio interface.

    Args:
        message: User's input message.
        history: Chat history from Gradio.

    Returns:
        Tuple of (empty string, updated history).
    """
    if not message.strip():
        return "", history

    # Get response from engine
    response = test_engine.chat(message)

    # Update history
    history.append([message, response])

    return "", history


def clear_chat() -> tuple[list[list[str]], str]:
    """Clear chat history.

    Returns:
        Tuple of (empty history, status message).
    """
    test_engine.clear_history()
    return [], "‚úÖ Chat history cleared!"


def get_stats() -> str:
    """Get current memory statistics.

    Returns:
        Markdown-formatted statistics.
    """
    msg_count = test_engine.get_message_count()
    max_msgs = test_engine.memory.max_messages
    utilization = (msg_count / max_msgs * 100) if max_msgs > 0 else 0

    stats = f"""
### üìä Memory Statistics

- **Messages in context:** {msg_count}/{max_msgs}
- **Model:** `{test_engine.model}`
- **Ollama host:** `{test_engine.ollama_host}`
- **Context utilization:** {utilization:.1f}%
- **Rolling window active:** {'Yes' if msg_count >= max_msgs else 'No'}

**Note:** This is a test interface with a mocked LLM backend.
    """
    return stats


def test_tool_time() -> str:
    """Test the get_current_time tool.

    Returns:
        JSON-formatted tool output.
    """
    result = get_current_time()
    data = json.loads(result)

    formatted = f"""
### üïê get_current_time() Result

```json
{json.dumps(data, indent=2)}
```

**Parsed:**
- **ISO Format:** {data['iso_format']}
- **Readable:** {data['readable']}
- **Timezone:** {data['timezone']}
    """
    return formatted


def test_tool_search(query: str, max_results: int) -> str:
    """Test the web_search tool.

    Args:
        query: Search query.
        max_results: Maximum number of results.

    Returns:
        Markdown-formatted search results.
    """
    if not query.strip():
        return "‚ö†Ô∏è Please enter a search query."

    result = web_search(query, max_results)
    data = json.loads(result)

    formatted = f"""
### üîç web_search() Result

**Query:** "{data['query']}"  
**Results found:** {data['results_count']}

"""

    for i, res in enumerate(data["results"], 1):
        formatted += f"""
---

**{i}. [{res['title']}]({res['url']})**

{res['snippet']}
"""

    if "note" in data:
        formatted += f"\n\n**Note:** {data['note']}"

    return formatted


def test_memory_operations(operation: str, num_messages: int, max_window: int) -> str:
    """Test memory operations with various configurations.

    Args:
        operation: Operation to perform.
        num_messages: Number of messages to add.
        max_window: Maximum window size.

    Returns:
        Test results as markdown.
    """
    memory = RollingMemory(max_messages=max_window)

    if operation == "Fill to capacity":
        for i in range(num_messages):
            memory.add_message("user", f"Test message {i + 1}")

        context = memory.get_context()
        content = "\n".join([f"- {msg['role']}: {msg['content']}" for msg in context])

        result = f"""
### üìù Memory Test: {operation}

**Configuration:**
- Max window size: {max_window}
- Messages added: {num_messages}
- Messages retained: {memory.message_count()}

**Context content:**
{content}

**Result:** {'‚úÖ Rolling worked correctly!' if memory.message_count() <= max_window else '‚ùå Window size exceeded!'}
        """

    elif operation == "Test rolling behavior":
        memory.set_system_message("System prompt for testing")

        for i in range(num_messages):
            memory.add_message("user", f"Message {i + 1}")

        context = memory.get_context()
        has_system = context[0]["role"] == "system" if context else False
        oldest_msg = context[1]["content"] if len(context) > 1 else "N/A"

        result = f"""
### üîÑ Memory Test: {operation}

**Configuration:**
- Max window size: {max_window}
- Messages added: {num_messages}
- Messages retained: {memory.message_count()}

**Results:**
- System message preserved: {'‚úÖ Yes' if has_system else '‚ùå No'}
- Oldest message in context: `{oldest_msg}`
- Total context size: {len(context)} (system + {memory.message_count()} messages)

**Expected oldest:** Message {max(1, num_messages - max_window + 1)} (if rolling occurred)
        """

    else:  # Clear and verify
        memory.set_system_message("System prompt")
        for i in range(num_messages):
            memory.add_message("user", f"Message {i + 1}")

        before_count = memory.message_count()
        memory.clear()
        after_count = memory.message_count()

        context = memory.get_context()
        has_system = len(context) == 1 and context[0]["role"] == "system"

        result = f"""
### üóëÔ∏è Memory Test: {operation}

**Configuration:**
- Messages before clear: {before_count}
- Messages after clear: {after_count}
- System message preserved: {'‚úÖ Yes' if has_system else '‚ùå No'}

**Result:** {'‚úÖ Clear worked correctly!' if after_count == 0 and has_system else '‚ùå Issue detected!'}
        """

    return result


def create_interface() -> gr.Blocks:
    """Create the Gradio web interface.

    Returns:
        Gradio Blocks interface.
    """
    with gr.Blocks(
        title="brAIniac Test Suite",
    ) as interface:
        gr.Markdown("""
        # üß™ brAIniac Interactive Test Suite
        
        Test all components of the brAIniac chatbot system in your browser.
        
        **Note:** This interface uses a **mocked LLM backend** for testing purposes. 
        No actual Ollama connection is required.
        """)

        with gr.Tabs():
            # Tab 1: Chat Interface
            with gr.Tab("üí¨ Chat Engine"):
                gr.Markdown("### Test the ChatEngine with rolling memory")

                chatbot = gr.Chatbot(
                    label="brAIniac Test Chat",
                    height=400,
                    show_label=True,
                )

                with gr.Row():
                    msg_input = gr.Textbox(
                        label="Your message",
                        placeholder="Type a message and press Enter...",
                        scale=4,
                    )
                    send_btn = gr.Button("Send", variant="primary", scale=1)

                with gr.Row():
                    clear_btn = gr.Button("üóëÔ∏è Clear History")
                    stats_btn = gr.Button("üìä Show Stats")

                status_output = gr.Markdown()

                # Event handlers
                msg_input.submit(
                    test_chat, inputs=[msg_input, chatbot], outputs=[msg_input, chatbot]
                )
                send_btn.click(
                    test_chat, inputs=[msg_input, chatbot], outputs=[msg_input, chatbot]
                )
                clear_btn.click(clear_chat, outputs=[chatbot, status_output])
                stats_btn.click(get_stats, outputs=[status_output])

            # Tab 2: Tool Testing
            with gr.Tab("üõ†Ô∏è Tool Testing"):
                gr.Markdown("### Test individual MCP tools")

                with gr.Row():
                    with gr.Column():
                        gr.Markdown("#### üïê get_current_time()")
                        time_btn = gr.Button("Execute Tool", variant="primary")
                        time_output = gr.Markdown()

                        time_btn.click(test_tool_time, outputs=[time_output])

                    with gr.Column():
                        gr.Markdown("#### üîç web_search()")
                        search_query = gr.Textbox(
                            label="Search Query",
                            placeholder="Enter search query...",
                            value="Python testing best practices",
                        )
                        search_max = gr.Slider(
                            label="Max Results",
                            minimum=1,
                            maximum=10,
                            value=5,
                            step=1,
                        )
                        search_btn = gr.Button("Execute Search", variant="primary")
                        search_output = gr.Markdown()

                        search_btn.click(
                            test_tool_search,
                            inputs=[search_query, search_max],
                            outputs=[search_output],
                        )

            # Tab 3: Memory Testing
            with gr.Tab("üíæ Memory Testing"):
                gr.Markdown("### Test RollingMemory behavior")

                with gr.Row():
                    memory_op = gr.Radio(
                        choices=[
                            "Fill to capacity",
                            "Test rolling behavior",
                            "Clear and verify",
                        ],
                        label="Test Operation",
                        value="Fill to capacity",
                    )

                with gr.Row():
                    memory_num_msgs = gr.Slider(
                        label="Number of Messages",
                        minimum=1,
                        maximum=50,
                        value=10,
                        step=1,
                    )
                    memory_max_window = gr.Slider(
                        label="Max Window Size",
                        minimum=1,
                        maximum=30,
                        value=5,
                        step=1,
                    )

                memory_btn = gr.Button("Run Memory Test", variant="primary")
                memory_output = gr.Markdown()

                memory_btn.click(
                    test_memory_operations,
                    inputs=[memory_op, memory_num_msgs, memory_max_window],
                    outputs=[memory_output],
                )

            # Tab 4: System Info
            with gr.Tab("‚ÑπÔ∏è System Info"):
                gr.Markdown(f"""
                ### System Information

                **brAIniac Version:** 0.1.0 (Phase 1)
                **Test Interface:** Gradio Web UI
                **Python Version:** {sys.version.split()[0]}
                **Test Mode:** Mock LLM Backend

                ---

                ### Component Status
                
                | Component | Status | Notes |
                |-----------|--------|-------|
                | RollingMemory | ‚úÖ Active | Rolling window working |
                | ChatEngine | ‚úÖ Active | Using mock Ollama client |
                | get_current_time | ‚úÖ Active | Returns system time |
                | web_search | ‚ö†Ô∏è Mock | Returns mock search results |
                | Ollama Connection | ‚ö†Ô∏è Mocked | Not connected to real Ollama |
                
                ---
                
                ### Test Coverage
                
                Run full test suite from terminal:
                ```bash
                poetry run pytest
                poetry run pytest --cov
                poetry run pytest -v --cov-report=html
                ```
                
                ---
                
                ### Quick Test Scenarios
                
                1. **Chat Flow:** Go to Chat Engine tab, send 5-10 messages, check stats
                2. **Rolling Memory:** Send 25+ messages with default config, verify oldest roll off
                3. **Tool Execution:** Test both tools in Tool Testing tab
                4. **Memory Stress Test:** Memory Testing tab, add 50 messages with window size 5
                """)

        gr.Markdown("""
        ---
        
        **brAIniac** - Local-first, uncensored AI chatbot optimized for 8GB VRAM  
        üìö [Documentation](https://github.com/yourusername/brAIniac) | üêõ [Report Issues](https://github.com/yourusername/brAIniac/issues)
        """)

    return interface


def main() -> None:
    """Launch the web test interface."""
    interface = create_interface()
    interface.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False,
        show_error=True,
    )


if __name__ == "__main__":
    main()
